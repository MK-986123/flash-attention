name: "Build FlashAttention2 Windows (Py312, Torch 2.8.0 + cu128, Ada Lovelace)"

on:
  workflow_dispatch:

jobs:
  build_wheels:
    name: Build wheels and Upload
    runs-on: windows-latest
    timeout-minutes: 300
    strategy:
      fail-fast: false
      matrix:
        flash-attn-version: ["2.8.3"]
        python-version: ["3.12"]
        torch-version: ["2.8.0"]
        cuda-version: ["12.8.1"]

    steps:
      - uses: actions/checkout@v4

      - name: Enable Git long paths
        run: git config --system core.longpaths true
        shell: pwsh

      - name: Enable MSVC Developer Command Prompt
        uses: ilammy/msvc-dev-cmd@v1
        with:
          arch: x64

      - name: Install CUDA Toolkit ${{ matrix.cuda-version }}
        uses: Jimver/cuda-toolkit@v0.2.24
        with:
          cuda: ${{ matrix.cuda-version }}
          method: "network"
          use-github-cache: false

      - name: Set CUDA and PyTorch versions (for naming)
        run: |
          $cudaVersion = "${{ matrix.cuda-version }}" -replace '\.', ''
          $cudaVersion = $cudaVersion.Substring(0, [Math]::Min(3, $cudaVersion.Length))
          echo "MATRIX_CUDA_VERSION=$cudaVersion" >> $env:GITHUB_ENV
          $torchVersion = "${{ matrix.torch-version }}" -replace '^(\d+\.\d+).*', '$1'
          echo "MATRIX_TORCH_VERSION=$torchVersion" >> $env:GITHUB_ENV
        shell: pwsh

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Upgrade build tooling
        run: |
          python -m pip install -U pip setuptools wheel
          pip install ninja cmake packaging
        shell: pwsh

      - name: Install PyTorch ${{ matrix.torch-version }} + cu${{ env.MATRIX_CUDA_VERSION }}
        run: |
          pip install --no-cache-dir --index-url https://download.pytorch.org/whl/cu${{ env.MATRIX_CUDA_VERSION }} torch==${{ matrix.torch-version }}
          nvcc --version
          where nvcc
          cl 2>$null || ver
          python -V
          python -c "import torch; print('PyTorch:', torch.__version__); print('CUDA (reported by torch):', torch.version.cuda); print('Arch list:', torch.cuda.get_arch_list())"
        shell: pwsh

      - name: Checkout flash-attn v${{ matrix.flash-attn-version }}
        run: |
          git clone https://github.com/Dao-AILab/flash-attention.git -b "v${{ matrix.flash-attn-version }}"
        shell: pwsh

      - name: Build FlashAttention2 wheel (Ada Lovelace sm_89)
        working-directory: flash-attention
        env:
          FORCE_CUDA: "1"
          TORCH_CUDA_ARCH_LIST: "8.9"
          CUDA_HOME: "${{ env.CUDA_PATH }}"
          CUDA_PATH: "${{ env.CUDA_PATH }}"
          MAX_JOBS: "2"
          NVCC_THREADS: "2"
          FLASH_ATTENTION_FORCE_BUILD: "TRUE"
        run: |
          $pyCode = @"
from torch.utils import cpp_extension
print("cpp_extension.CUDA_HOME:", cpp_extension.CUDA_HOME)
"@
          python -c "$pyCode"
          python setup.py bdist_wheel --dist-dir=dist
          $baseWheelName = Get-ChildItem -Path "dist\*.whl" | Select-Object -First 1 | ForEach-Object { $_.Name }
          if (-not $baseWheelName) { throw "No wheel produced in dist" }
          $wheelName = $baseWheelName.Replace("${{ matrix.flash-attn-version }}", "${{ matrix.flash-attn-version }}+cu$env:MATRIX_CUDA_VERSION" + "torch$env:MATRIX_TORCH_VERSION")
          Move-Item "dist\$baseWheelName" "dist\$wheelName"
          echo "wheel_name=$wheelName" >> $env:GITHUB_ENV
        shell: pwsh

      - name: Install test (import flash_attn)
        run: |
          pip install flash-attention/dist/$env:wheel_name
          python -c "import flash_attn, sys; print('flash_attn version:', getattr(flash_attn, '__version__', 'unknown')); print('Python:', sys.version)"
        shell: pwsh

      - name: Upload wheel artifact
        uses: actions/upload-artifact@v4
        with:
          name: flash_attn_win_py312_cu${{ env.MATRIX_CUDA_VERSION }}_torch${{ env.MATRIX_TORCH_VERSION }}
          path: flash-attention/dist/*.whl
